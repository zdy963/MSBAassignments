---
title: "Assignment_1"
author: "Dongyu Zhang"
date: "1/24/2019"
output:
  html_document:
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(polywog)
library(caret)
library(boot)
```

# Question 1
## a.
When $x ≤ \xi$, $(x-\xi) ≤ 0$. So, $f(x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3$.<br>
According to question, $f_1(x) = f(x)$. Thus,
$$
\begin{aligned}
a_1 = \beta_0 \\
b_1 = \beta_1 \\
c_1 = \beta_2 \\
d_1 = \beta_3
\end{aligned}
$$

## b.
When $x > \xi$, $(x-\xi) > 0$. So $f(x)$ becomes:
$$
\begin{aligned}
f(x) =&\, \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x-\xi)^3 \\
=&\, \beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+\beta_4(x^3 - 3x^2\xi + 3x\xi^2 - \xi^3) \\
=&\, (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)x 
    + (\beta_2 - 3\beta_4\xi)x^2 + (\beta_3 + \beta_4)x^3
\end{aligned}
$$
According to question, $f_2(x)=f(x)$. Therefore,
$$
\begin{aligned}
a_2 =&\, \beta_0 - \beta_4\xi^3 \\
b_2 =&\, \beta_1 + 3\beta_4\xi^2 \\
c_2 =&\, \beta_2 - 3\beta_4\xi \\
d_2 =&\, \beta_3 + \beta_4
\end{aligned}
$$

## c.
For $f_1(\xi)$
$$
\begin{aligned}
f_1(x) =&\, \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 \\
f_1(\xi) =&\, \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3 \\
\end{aligned}
$$

For $f_2(\xi)$
$$
\begin{aligned}
f_2(x)   =&\, (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)x 
    + (\beta_2 - 3\beta_4\xi)x^2 + (\beta_3 + \beta_4)x^3 \\
f_2(\xi) =&\, (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)\xi 
    + (\beta_2 - 3\beta_4\xi)\xi^2 + (\beta_3 + \beta_4)\xi^3 \\
f_2(\xi) =&\, \beta_0 - \beta_4\xi^3 + \beta_1\xi + 3\beta_4\xi^3 
    + \beta_2\xi^2 - 3\beta_4\xi^3 + \beta_3\xi^3 + \beta_4\xi^3 \\
f_2(\xi) =&\, \beta_0 + \beta_1\xi + \beta_2\xi^2 + \beta_3\xi^3 \\
         =&\, f_1(\xi)
\end{aligned}
$$

## d.
For $f_1'(\xi)$:
$$
\begin{aligned}
f_1(x) =&\, \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 \\
f_1'(x) =&\, \beta_1 + 2\beta_2x + 3\beta_3x^2 \\
f_1'(\xi) =&\, \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 \\
\end{aligned}
$$

For $f_2'(\xi)$
$$
\begin{aligned}
f_2(x) =&\, (\beta_0 - \beta_4\xi^3) + (\beta_1 + 3\beta_4\xi^2)x 
    + (\beta_2 - 3\beta_4\xi)x^2 + (\beta_3 + \beta_4)x^3 \\
f_2'(x) =&\, (\beta_1 + 3\beta_4\xi^2) + 2(\beta_2 - 3\beta_4\xi)x
          + 3(\beta_3 + \beta_4)x^2 \\
f_2'(\xi) =&\, (\beta_1 + 3\beta_4\xi^2) + 2(\beta_2 - 3\beta_4\xi)\xi
          + 3(\beta_3 + \beta_4)\xi^2 \\
        =&\, \beta_1 + 3\beta_4\xi^2 + 2\beta_2\xi - 6\beta_4\xi^2
          + 3\beta_3\xi^2  + 3\beta_4\xi^2 \\
        =&\, \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 \\
        =&\, f_1'(x)
\end{aligned}
$$

## e.
For $f_1''(\xi)$:
$$
\begin{aligned}
f_1'(x) =&\, \beta_1 + 2\beta_2x + 3\beta_3x^2 \\
f_1''(x) =&\, 2\beta_2 + 6\beta_3x \\
f_1''(\xi) =&\, 2\beta_2 + 6\beta_3\xi \\
\end{aligned}
$$

For $f_2''(\xi)$
$$
\begin{aligned}
f_2'(x) =&\, (\beta_1 + 3\beta_4\xi^2) + 2(\beta_2 - 3\beta_4\xi)x
          + 3(\beta_3 + \beta_4)x^2 \\
f_2''(x) =&\, 2(\beta_2 - 3\beta_4\xi) + 6(\beta_3 + \beta_4)x \\
f_2''(\xi) =&\, 2(\beta_2 - 3\beta_4\xi) + 6(\beta_3 + \beta_4)\xi \\
f_2''(\xi) =&\, 2\beta_2 - 6\beta_4\xi + 6\beta_3\xi + 6\beta_4\xi) \\
f_2''(\xi) =&\, 2\beta_2 + 6\beta_3\xi \\
           =&\, f_1''(\xi)
\end{aligned}
$$

# Question 2
```{r}
x=seq(-2,2)
y=ifelse(x<1,1,2-x)
plot(x,y,type='l')
```
$$
Intercept: 1\\
Slop: 
\begin{cases}
0,\;-2≤x<1&\\
-1,\;1≤x≤2&
\end{cases}
$$

# Question 3
For $g^{(m)}$, it does not hava any effect on polynomial with order lower than $m$. So, when $m$ becomes higher, the effectiveness of panelty becomes lower and, thus, curve is more volatile, which means lower bias but higher variance.

## a.
When $\lambda \rightarrow \infty$, because higher the $m$ is, lower the bias are. Thus, $\hat g_2$ has smaller training RSS.

## b.
When $\lambda \rightarrow \infty$, because smaller the $m$ is, lower the variance are. Thus, $\hat g_1$ has smaller test RSS.

## c.
When $\lambda=0$, the panelty does not have any effect on function, which means $\hat g^{(m)}$ can be any function that interpolates the data. So, $\hat g_1$ and $\hat g_2$ have same test and training RSS.


# Question 4
```{r}
graphData <- function(fit){
  plot(wage~age, data=Wage, col="darkgrey")
  agelims=range(age)
  age.grid=seq(from=agelims[1], to=agelims[2])
  preds <- predict(fit, newdata=list(age=age.grid))
  lines(age.grid, preds, lwd=2, col="blue")
}
```
## a.
```{r}
Wage <- read.table("./../data/wage.txt")
attach(Wage)

delta<-rep(NA,10)
set.seed(168)
for (degree in c(1:5)){
  fit <- glm(wage~poly(age,degree), data=Wage)
  delta[degree] <- cv.glm(Wage, fit, K=10)$delta[2]
}
which.min(delta)
```
The optimal degree for polynomial that cv found is $4$.

```{r}
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
```
The result anova found is 2, which is quite different from what cv found.

```{r}
fit <- lm(wage~poly(age,2), data=Wage)
graphData(fit)
```

## b.
```{r}
delta<-rep(NA,10)
set.seed(168)
for (nfolds in c(2:10)){
  Wage$age.fold <- cut(Wage$age, nfolds)
  fit <- glm(wage~age.fold, data=Wage)
  delta[nfolds] <- cv.glm(Wage, fit, K=10)$delta[1]
}
which.min(delta)
```

Using cross-validation, we found that 8 folds is the optimal number of cuts. So we use 8 folds to fit a step function of data and predict.

```{r}
fit <- lm(wage ~ cut(age, 8), data=Wage)
graphData(fit)
```










