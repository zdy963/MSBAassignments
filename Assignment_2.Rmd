---
title: "Assignment_2"
author: "Dongyu Zhang"
date: "12/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 6)
library(fpp2)
library(TSA)
```

## Question 1
```{r}
hwi.new <- read.table('../../Data/hawaii-new.dat')
total <- ts(hwi.new$V2,frequency=12, start=c(1970,1))
total.log <- log(total)
```

### f.
```{r}
par(mfrow=c(3,1))
total.season <- diff(total.log, lag = 12)
total.diff <- diff(total.log)
plot(total.season, main = "Take seasonal Differencing",ylab="r")
plot(diff(total.season), main = "Take another Differencing",ylab="r")
plot(diff(diff(total.season)), main = "Take one more Differencing",ylab="r")
```
<br>Differencing after taking a seasonal differencing twice looks like a white noise. Recalling that we used a quadratic model to fit the log transformed total series and cubic model is not better than squared one. So, toke twice differencing after seasonal differencing is enough to make the log series stationary.
### g.
```{r}
total.ma <- filter(total,c(0.5,rep(1,11),0.5)/12,sides = 1)
plot(total,ylab="Number of people",main = "Original and smoothed series")
lines(as.vector(time(total.ma)),total.ma,type = 'l',col='blue')
```
<br>From the plot, we can see a very clear trend of increasing and a decrease around 1992, then started growing at 1995. Also, the benefit of moving average is very significant, which made the series more flat but keep the trend.


### h.
```{r}
total.se <- colMeans(matrix(total - total.ma,ncol=12,byrow=TRUE),na.rm=TRUE)
total.se <- total.se - mean(total.se)
res<- data.frame(month=month.abb[1:12],total.se)
res
```

### i.
```{r}
n <- length(total)
total.deseason <- total - rep(total.se,n/12)
total.deseason.mv <- filter(c(rep(total.deseason[1],2),total.deseason,rep(total.deseason[n],2)),rep(1,13)/13,sides=1)[-c(1:2,n+3:4)]
m_hat <- total.deseason.mv + rep(total.se,n/12)
plot(as.vector(time(total)),m_hat,type = "l",col="black",xlab="Time",ylab="Number of People")
lines(as.vector(time(total)),total,type="l",col="blue",lty=2)
```
###j.
```{r}
total.deseason.detrend <- total.deseason - total.deseason.mv
total.season.quad = tslm(total.log~trend+I(trend^2) + season-1)
par(mfrow=c(2,1))
plot(total.deseason.detrend,type = "l",ylab="Residuals",main="De-trended and de_seasoned series")
abline(h=0)
plot(total.season.quad$residuals,ylab="Residuals",main="Plot from Part(d)")
abline(h=0)
```
<br>Comparing with residual generated by regression, residual generated by moving average seems flater and does not have startup problem. However, the valley around 1992 in first plot is more significant than that in second plot, which means moving average might miss some important value by using average to summarize this period and causes huge error.


## Question 2
```{r}
retaildata <- readxl::read_excel("../../Data/retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349639C"],
  frequency=12, start=c(1982,4))

myts.train <- window(myts, end=c(2010,12))
myts.test <- window(myts, start=2011)
autoplot(myts) +
  autolayer(myts.train, series="Training") +
  autolayer(myts.test, series="Test")
fc <- snaive(myts.train)
checkresiduals(fc)
```
From the residual plot, we can see that it is nearly normal distributed excepted for the outlier. However, when we look into result from Ljung-Box test, we can see that its p-value is still very small not large enough to refuse the H0 hypothesis. So, it is still correlated with time. The ACF plot also prove our asumption.<br>

## Question 3
```{r}

train1 <- window(visnights[, "QLDMetro"], end = c(2015, 4))
train2 <- window(visnights[, "QLDMetro"], end = c(2014, 4))
train3 <- window(visnights[, "QLDMetro"], end = c(2013, 4))

fc1 <- snaive(train1,h=5)
fc2 <- snaive(train2,h=5)
fc3 <- snaive(train3,h=5)

autoplot(window(visnights[, "QLDMetro"])) + 
  autolayer(fc1,series = "fc1",PI=FALSE) + 
  autolayer(fc2,series = "fc2",PI=FALSE) + 
  autolayer(fc3,series = "fc3",PI=FALSE)

accuracy(fc1,window(visnights[, "QLDMetro"]))
accuracy(fc2,train1)
accuracy(fc3,train2)
```

## Question 4
### a.
```{r}
autoplot(books)
```
<br>From the plot, paperback and hardcover seems to have a opposite cycle at the begining of the month and tend to become same since the mid of the month. Hardcover starts from a lower point but exceed paperback after 10 and keep almost always higher than paperback to the end of the month.

### b. Expotential Smoothing
```{r}
pb <- window(books[,'Paperback'])
hc <- window(books[,'Hardcover'])
fcp <- ses(pb,h=4)
fch <- ses(hc,h=4)
autoplot(books) + 
  autolayer(fcp,PI=FALSE,series = 'Forecast for Paperback') + 
  autolayer(fch,PI=FALSE,series = 'Forecast for Hardcover')
```
    
###c. Accuracy of Expotential method
```{r}
accuracy(fcp)
accuracy(fch)
```
   
### 6.a Holt's method
```{r}
hpb <- holt(pb,h=4)
hhc <- holt(hc,h=4)
autoplot(books) + 
  autolayer(hpb,PI=FALSE,series = "Holt's line for Paperback") + 
  autolayer(hhc,PI=FALSE,series = "Holt's line for Hardcover")
```
    
### 6.b
```{r}
accuracy(hpb)
accuracy(fcp)
accuracy(hhc)
accuracy(fch)
```
<br>According to RMSE, holt's method is better.

## Question 5
### a.
$$
\begin{aligned}
&(1-.3B)(1-.5B)(1-.6B+.4B^2)(x_t-.5)\\
=&(1-.8B+.15B^2)(1-.6B+.4B^2)(x_t-.5)\\
=&(1-.6B+.4B^2-.8B+.48B^2-.32B^3+.15B^2-.09B^3+.06B^4)(x_t-.5)\\
=&(1-1.4B+1.03B^2-.41B^3+.06B^4)(x_t-.5)\\
=&x_t-1.4x_{t-1}+1.03x_{t-2}-.41x_{t-3}+.06x_{t-4} - .14\\
\Rightarrow\epsilon_t=&(1-.3B)(1-.5B)(1-.6B+.4B^2)(x_t-.5)
\end{aligned}
$$
     
### b.
$$
\begin{aligned}
(1-.3B)(1+.8B)\epsilon_t=&\epsilon_t + .5\epsilon_{t-1} - .24\epsilon_{t-2}\\
(1-.3B)(1-.5B)(1-.6B+.4B^2)(x_t-.5) =& (1-.3B)(1-.5B)(1-.6B+.4B^2)(x_t-.5)\\
=&\epsilon_t + .5\epsilon_{t-1} - .24\epsilon_{t-2}
\end{aligned}
$$

## Question 6
### Mean
$$
\begin{aligned}
\mu = &E(0.01 + 0.2x_{t-2} + \epsilon_t)\\
=& 0.01 + 0.2\mu + 0\\
Therefore,\\
\mu =& 0.0125
\end{aligned}
$$
     
### Variance
$$
\begin{aligned}
Var(x_t) =& Var(0.01 + 0.2x_{t-2} + \epsilon_t)\\
=& 0 + 0.2^2Var(x_t) + 0.02\\
Therefore,\\
Var(x_t) =& 0.02083
\end{aligned}
$$
     
### Autocorrelations
$$
\begin{aligned}
\gamma_0 =& Var(x_t) = 0.02083\\
\gamma_1 =& Cov(x_t,x_{t-1})\\
=& Cov(0.01 + 0.2x_{t-2} + \epsilon_t, x_{t-1})\\
=& 0 + 0.2Cov(x_{t-2},x_{t-1}) + 0\\
=& 0.2\gamma_1\\
\Rightarrow& \gamma_1 = 0\\
\\
\gamma_2 =& Cov(x_t,x_{t-2})\\
=& Cov(0.01 + 0.2x_{t-2} + \epsilon_t,x_{t-2})\\
=& 0 + \gamma_0 + 0\\
=& 0.02083\\
\gamma_h =& Cov(x_t,x_{t-h})\\
\\
=& Cov(0.01 + 0.2x_{t-2} + \epsilon_t,x_{h})\\
=& 0 + 0.2Cov(x_{t-2},x_{t-h}) + 0\\
=& 0.2\gamma_{h-2}\\
\\
\rho_0 =& 1\\
\rho_1 =& \frac{\gamma_1}{\gamma_0}= 0\\
\rho_h =& \frac{\gamma_h}{\gamma_0}=\frac{0.2\gamma_{h-2}}{\gamma_0}=0.2\rho_{h-2}\\
=&
\begin{cases}
0& \text{h is odd}\\
0.2^\frac{h}{2}& \text{h is even}\\
\end{cases}
\end{aligned}
$$

## Question 7
### 1.
```{r}
T <- 600
ar3 <- arima.sim(model=list(ar=c(0.8,-0.5,-0.2)),n=T,innov=rnorm(n=600,mean=0,sd=1),mean=1/3)
ma3 <- arima.sim(model=list(ma=c(0.8,-0.5,-0.2)),T,innov=rnorm(n=600,mean=0,sd=1),mean=0.3)
arma3.2 <- arima.sim(model=list(ar=c(0.8,-0.5,-0.2),ma=c(0.5,0.3)),T,innov=rnorm(n=600,mean=0,sd=1),mean=1/3)

par(mfrow=c(3,1))
plot(ar3, main = "Seires of AR(3)",ylab="r")
plot(ma3, main = "Series of MA(3)",ylab="r")
plot(arma3.2, main = "Series of ARMA(3,2)",ylab="r")
```

### 2.
```{r}
par(mfrow=c(2,2))
acf(ar3,lag=15, main = "ACF of AR(3)")
acf(ma3,lag=15, main = "ACF of MA(3)")
acf(arma3.2,lag=15, main = "ACF of ARMA(3,2)")
```

### 3.
```{r}
par(mfrow=c(2,2))
pacf(ar3,lag=15, main = "PACF of AR(3)")
pacf(ma3,lag=15, main = "PACF of MA(3)")
pacf(arma3.2,lag=15, main = "PACF of ARMA(3,2)")
```
###3.
```{r}
eacf(ar3)
eacf(ma3)
eacf(arma3.2)
```
###4.
```{r}
par(mfrow=c(2,2))
plot(ARMAtoMA(ar=c(0.8,-0.5,-0.2),lag.max = 10),type = "h", main = "Memory function of AR(3)",ylab="psi",xlab="lag")
abline(h=0)
plot(ARMAtoMA(ma=c(0.8,-0.5,-0.2),lag.max = 10),type = "h", main = "Memory function of MA(3)",ylab="psi",xlab="lag")
abline(h=0)
plot(ARMAtoMA(ar=c(0.8,-0.5,-0.2),ma=c(0.5,0.3),lag.max = 10),type = "h", main = "Memory function of ARMA(3,2)",ylab="psi",xlab="lag")
abline(h=0)
```